{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据集&预处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "file_path = 'dataset/FTD_90_200_fMRI.mat'\n",
    "\n",
    "labels=['FTD', 'NC']\n",
    "\n",
    "label_mapping = {\n",
    "    \"FTD\": 0,\n",
    "    \"NC\": 1\n",
    "}\n",
    "\n",
    "# 使用loadmat函数读取.mat文件\n",
    "data = loadmat(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data格式\n",
    "data={  \n",
    "    'FTD':array(95, 90, 200),  \n",
    "    'NC':array(86, 90, 200),  \n",
    "}  \n",
    "转换为  \n",
    "data:(181,90,200)  \n",
    "label:(181,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尚未归一化  \n",
    "后续：  \n",
    "1.对于 同一样本的 同一脑区 的数据 进行归一化  \n",
    "2.对于 不同样本间的 同一脑区的 同一时间步的数据 进行归一化  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "ftd = data['FTD']\n",
    "nc = data['NC']\n",
    "\n",
    "\n",
    "all_labels = np.concatenate((np.zeros(ftd.shape[0]), np.ones(nc.shape[0])))\n",
    "\n",
    "data_combined = np.vstack((ftd,nc))\n",
    "\n",
    "data_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 归一化1  \n",
    "对于 同一样本的 同一脑区 的数据 进行归一化  \n",
    "data_combined[i]转置后 列是脑区序列随时间步变化的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm1 = np.zeros(data_combined.shape)\n",
    "print(data_norm1.shape)\n",
    "print(data_norm1[0].shape)\n",
    "for i in range(data_combined.shape[0]):\n",
    "    data_norm1[i] = MinMaxScaler().fit_transform(data_combined[i].T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined[0].T.shape   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 归一化2  \n",
    "对于 不同样本间的 同一脑区的 同一时间步的数据 进行归一化  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm2 = np.zeros(data_combined.shape)\n",
    "for region in range(data_combined.shape[1]):\n",
    "    for time_step in range(data_combined.shape[2]):\n",
    "        data_norm2[:, region, time_step] = MinMaxScaler().fit_transform(data_combined[:, region, time_step].reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined[:, region, time_step].reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 归一化3  \n",
    "所有样本 同一时间步的数据 进行归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm3 = np.zeros(data_combined.shape)\n",
    "data_norm3 = MinMaxScaler().fit_transform(data_combined.reshape(-1, data_combined.shape[-1])).reshape(data_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined.reshape(-1, data_combined.shape[-1]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 归一化4  \n",
    "每一个样本 不同脑区的同一时间步的数据 进行归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm4 = np.zeros(data_combined.shape)\n",
    "for sample in range(data_combined.shape[0]):\n",
    "    for time_step in range(data_combined.shape[2]):\n",
    "        data_norm4[sample, :, time_step] = MinMaxScaler().fit_transform(data_combined[sample, :, time_step].reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined[0, :, 0].reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 散点图  \n",
    "0号样本0号脑区随时间步变化的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data_combined[0][0]\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(20, 10))\n",
    "\n",
    "axs[0, 0].scatter(range(len(d)), d)\n",
    "axs[0, 0].set_title('Original Data')\n",
    "\n",
    "axs[0, 1].scatter(range(len(d)), data_norm1[0][0])\n",
    "axs[0, 1].set_title('Normalized Data 1')\n",
    "\n",
    "axs[1, 0].scatter(range(len(d)), data_norm2[0][0])\n",
    "axs[1, 0].set_title('Normalized Data 2')\n",
    "\n",
    "axs[1, 1].scatter(range(len(d)), data_norm3[0][0])\n",
    "axs[1, 1].set_title('Normalized Data 3')\n",
    "\n",
    "axs[2, 0].scatter(range(len(d)), data_norm4[0][0])\n",
    "axs[2, 0].set_title('Normalized Data 4')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "num_samples = data_combined.shape[0]  \n",
    "\n",
    "random_sample_indices = list(range(num_samples))\n",
    "random.shuffle(random_sample_indices)\n",
    "\n",
    "\n",
    "for sample_index in random_sample_indices[:10]:\n",
    "    l = all_labels[sample_index]\n",
    "    sample = data_combined[sample_index][0]\n",
    "    sample = sample.reshape(1,200)\n",
    "    \n",
    "    plt.figure(figsize=(80, 20))\n",
    "    plt.imshow(sample, cmap='gray')\n",
    "    plt.title(f\"Sample {sample_index} Gray-Scale Image\")\n",
    "    plt.xlabel(\"Width (200)\")\n",
    "    plt.ylabel(\"Height (90)\")\n",
    "    print(l)\n",
    "    \n",
    "    plt.show()\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_data = data_norm4\n",
    "\n",
    "data_T = np.zeros((use_data.shape[0], use_data.shape[2], use_data.shape[1]))\n",
    "for i in range(use_data.shape[0]):\n",
    "    data_T[i] = use_data[i].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder  \n",
    "减少脑区数量或时间步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "from tensorflow.keras.layers import Input,GRU\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "use_data = data_T\n",
    "\n",
    "# 定义Autoencoder模型\n",
    "input_shape = (use_data.shape[1], use_data.shape[2])\n",
    "input_layer = Input(shape=input_shape)\n",
    "encoded = GRU(64, activation='relu', return_sequences=True)(input_layer)\n",
    "encoded = GRU(45, activation='relu', return_sequences=True)(encoded)\n",
    "\n",
    "decoded = GRU(45, activation='relu', return_sequences=True)(encoded)\n",
    "decoded = GRU(64, activation='relu', return_sequences=True)(decoded)\n",
    "decoded = GRU(use_data.shape[2], activation='sigmoid', return_sequences=True)(decoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()\n",
    "\n",
    "# 训练Autoencoder模型\n",
    "autoencoder.fit(use_data, use_data, epochs=50, batch_size=32, shuffle=True)\n",
    "\n",
    "# 生成新的数据\n",
    "encoder = Model(input_layer, encoded)\n",
    "data_reduce = encoder.predict(use_data).reshape((181,45,200))\n",
    "print(data_reduce.shape)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分为训练和验证  \n",
    "在下面选择归一化方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Use_data = data_T\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(Use_data, all_labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, GRU, LSTM, Dense, Reshape, Bidirectional, Dropout, Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Reshape((X_train.shape[1], X_train.shape[2]), input_shape=(X_train.shape[1], X_train.shape[2], 1)))\n",
    "model.add(GRU(64, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GRU(128, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(256)))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# 编译模型\n",
    "optimizer = Adam(learning_rate=0.000001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习率调度器\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=2)\n",
    "y_val_one_hot = to_categorical(y_val, num_classes=2)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train_one_hot, \n",
    "    validation_data=(X_val, y_val_one_hot), \n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[reduce_lr] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练和验证的损失和准确率曲线\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

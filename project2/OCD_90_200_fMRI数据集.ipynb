{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "file_path = 'dataset/OCD_90_200_fMRI.mat'\n",
    "\n",
    "labels=['OCD', 'NC']\n",
    "\n",
    "label_mapping = {\n",
    "    \"OCD\": 0,\n",
    "    \"NC\": 1,\n",
    "}\n",
    "\n",
    "# 使用loadmat函数读取.mat文件\n",
    "data = loadmat(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data格式\n",
    "data={  \n",
    "    'OCD':array(62, 90, 200),   \n",
    "    'NC':array(20, 90, 200),  \n",
    "}  \n",
    "转换为  \n",
    "data:(82, 90, 200)  \n",
    "label:(82,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尚未归一化  \n",
    "后续：对于 不同样本间的 同一脑区的 同一时间的数据 进行归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "ocd = data['OCD']\n",
    "nc = data['NC']\n",
    "\n",
    "\n",
    "all_labels = np.concatenate((np.zeros(ocd.shape[0]), np.ones(nc.shape[0])))\n",
    "\n",
    "data_combined = np.vstack((ocd, nc))\n",
    "\n",
    "data_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分为训练和验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(data_combined, all_labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "展示一个样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=data_combined[0][0]\n",
    "\n",
    "plt.scatter(range(len(d)), d)\n",
    "plt.xlabel('Index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入基本库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "import os\n",
    "import urllib.request\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置标志和超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration flags and hyperparameters\n",
    "USE_MAMBA = 1\n",
    "DIFFERENT_H_STATES_RECURRENT_UPDATE_MECHANISM = 0\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义超参数和初始化  \n",
    "这里的超参数，如模型维度(d_model)、状态大小、序列长度和批大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 200\n",
    "state_size = 128  # Example state size\n",
    "seq_len = 90  # Example sequence length\n",
    "batch_size = 64  # Example batch size\n",
    "last_batch_size = 81  # only for the very last batch of the dataset\n",
    "current_batch_size = batch_size\n",
    "different_batch_size = False\n",
    "h_new = None\n",
    "temp_buffer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S6模块\n",
    "S6模块是Mamba架构中的一个复杂组件，负责通过一系列线性变换和离散化过程处理输入序列。它在捕获序列的时间动态方面起着关键作用，这是序列建模任务(如语言建模)的一个关键方面。这里包括张量运算和自定义离散化方法来处理序列数据的复杂需求。\n",
    "\n",
    "这个S6的模块，可以处理离散化过程和正向传播。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S6(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, state_size, device):\n",
    "        super(S6, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(d_model, d_model, device=device)\n",
    "        self.fc2 = nn.Linear(d_model, state_size, device=device)\n",
    "        self.fc3 = nn.Linear(d_model, state_size, device=device)\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        self.state_size = state_size\n",
    "\n",
    "\n",
    "        self.A = nn.Parameter(F.normalize(torch.ones(d_model, state_size, device=device), p=2, dim=-1))\n",
    "        nn.init.xavier_uniform_(self.A)\n",
    "\n",
    "        self.B = torch.zeros(batch_size, self.seq_len, self.state_size, device=device)\n",
    "        self.C = torch.zeros(batch_size, self.seq_len, self.state_size, device=device)\n",
    "\n",
    "        self.delta = torch.zeros(batch_size, self.seq_len, self.d_model, device=device)\n",
    "        self.dA = torch.zeros(batch_size, self.seq_len, self.d_model, self.state_size, device=device)\n",
    "        self.dB = torch.zeros(batch_size, self.seq_len, self.d_model, self.state_size, device=device)\n",
    "\n",
    "        # h  [batch_size, seq_len, d_model, state_size]\n",
    "        self.h = torch.zeros(batch_size, self.seq_len, self.d_model, self.state_size, device=device)\n",
    "        self.y = torch.zeros(batch_size, self.seq_len, self.d_model, device=device)\n",
    "\n",
    " \n",
    "    def discretization(self):\n",
    "\n",
    "        self.dB = torch.einsum(\"bld,bln->bldn\", self.delta, self.B)\n",
    "\n",
    "        self.dA = torch.exp(torch.einsum(\"bld,dn->bldn\", self.delta, self.A))\n",
    "\n",
    "\n",
    "        return self.dA, self.dB\n",
    "\n",
    "    def forward(self, x):\n",
    "    # Algorithm 2  MAMBA paper\n",
    "        self.B = self.fc2(x)\n",
    "        self.C = self.fc3(x)\n",
    "        self.delta = F.softplus(self.fc1(x))\n",
    "\n",
    "        self.discretization()\n",
    "\n",
    "        if DIFFERENT_H_STATES_RECURRENT_UPDATE_MECHANISM:  \n",
    "            global current_batch_size\n",
    "            current_batch_size = x.shape[0]\n",
    "\n",
    "            if self.h.shape[0] != current_batch_size:\n",
    "                different_batch_size = True\n",
    "                h_new =  torch.einsum('bldn,bldn->bldn', self.dA, self.h[:current_batch_size, ...]) + rearrange(x, \"b l d -> b l d 1\") * self.dB\n",
    "\n",
    "            else:\n",
    "                different_batch_size = False\n",
    "                h_new =  torch.einsum('bldn,bldn->bldn', self.dA, self.h) + rearrange(x, \"b l d -> b l d 1\") * self.dB\n",
    "\n",
    "                # y  [batch_size, seq_len, d_model]\n",
    "                self.y = torch.einsum('bln,bldn->bld', self.C, h_new)\n",
    "\n",
    "                global temp_buffer\n",
    "                temp_buffer = h_new.detach().clone() if not self.h.requires_grad else h_new.clone()\n",
    "\n",
    "                return self.y\n",
    "\n",
    "        else:  \n",
    "            # h [batch_size, seq_len, d_model, state_size]\n",
    "            h = torch.zeros(x.size(0), self.seq_len, self.d_model, self.state_size, device=x.device)\n",
    "            y = torch.zeros_like(x)\n",
    "\n",
    "            h =  torch.einsum('bldn,bldn->bldn', self.dA, h) + rearrange(x, \"b l d -> b l d 1\") * self.dB\n",
    "\n",
    "            # y  [batch_size, seq_len, d_model]\n",
    "            y = torch.einsum('bln,bldn->bld', self.C, h)\n",
    "\n",
    "            return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MambaBlock类\n",
    "MambaBlock类是一个定制的神经网络模块，被设计为Mamba模型的关键构建块。它封装了几个层和操作来处理输入数据。\n",
    "\n",
    "包括线性投影、卷积、激活函数、自定义S6模块和残差连接。该块是Mamba模型的基本组件，负责通过一系列转换处理输入序列，以捕获数据中的相关模式和特征。这些不同层和操作的组合允许MambaBlock有效地处理复杂的序列建模任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, state_size, device):\n",
    "        super(MambaBlock, self).__init__()\n",
    "\n",
    "        self.inp_proj = nn.Linear(d_model, 2*d_model, device=device)\n",
    "        self.out_proj = nn.Linear(2*d_model, d_model, device=device)\n",
    "\n",
    "        # For residual skip connection\n",
    "        self.D = nn.Linear(d_model, 2*d_model, device=device)\n",
    "\n",
    "        # Set _no_weight_decay attribute on bias\n",
    "        self.out_proj.bias._no_weight_decay = True\n",
    "\n",
    "        # Initialize bias to a small constant value\n",
    "        nn.init.constant_(self.out_proj.bias, 1.0)\n",
    "\n",
    "        self.S6 = S6(seq_len, 2*d_model, state_size, device)\n",
    "\n",
    "        # Add 1D convolution with kernel size 3\n",
    "        self.conv = nn.Conv1d(seq_len, seq_len, kernel_size=3, padding=1, device=device)\n",
    "\n",
    "        # Add linear layer for conv output\n",
    "        self.conv_linear = nn.Linear(2*d_model, 2*d_model, device=device)\n",
    "\n",
    "        # rmsnorm\n",
    "        self.norm = RMSNorm(d_model, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "                x_proj.shape = torch.Size([batch_size, seq_len, 2*d_model])\n",
    "                x_conv.shape = torch.Size([batch_size, seq_len, 2*d_model])\n",
    "                x_conv_act.shape = torch.Size([batch_size, seq_len, 2*d_model])\n",
    "                \"\"\"\n",
    "        # Refer to Figure 3 in the MAMBA paper\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x_proj = self.inp_proj(x)\n",
    "\n",
    "        # Add 1D convolution with kernel size 3\n",
    "        x_conv = self.conv(x_proj)\n",
    "\n",
    "        x_conv_act = F.silu(x_conv)\n",
    "\n",
    "        # Add linear layer for conv output\n",
    "        x_conv_out = self.conv_linear(x_conv_act)\n",
    "\n",
    "        x_ssm = self.S6(x_conv_out)\n",
    "        x_act = F.silu(x_ssm)  # Swish activation can be implemented as x * sigmoid(x)\n",
    "\n",
    "        # residual skip connection with nonlinearity introduced by multiplication\n",
    "        x_residual = F.silu(self.D(x))\n",
    "\n",
    "        x_combined = x_act * x_residual\n",
    "\n",
    "        x_out = self.out_proj(x_combined)\n",
    "\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mamba模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mamba(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, state_size, device):\n",
    "        super(Mamba, self).__init__()\n",
    "        self.mamba_block1 = MambaBlock(seq_len, d_model, state_size, device)\n",
    "        self.mamba_block2 = MambaBlock(seq_len, d_model, state_size, device)\n",
    "        self.mamba_block3 = MambaBlock(seq_len, d_model, state_size, device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mamba_block1(x)\n",
    "        x = self.mamba_block2(x)\n",
    "        x = self.mamba_block3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaClassifier(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, state_size, num_classes, device):\n",
    "        super(MambaClassifier, self).__init__()\n",
    "        self.mamba = Mamba(seq_len, d_model, state_size, device)\n",
    "        self.fc = nn.Linear(d_model, num_classes)  # 分类层\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mamba(x)\n",
    "        x = x[:, -1, :]  # 取最后一时刻的输出\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSNorm  \n",
    "RMSNorm是一个自定义规范化层，这一层用于规范神经网络的激活，这可以帮助稳定和加快训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,\n",
    "        d_model: int,\n",
    "        eps: float = 1e-5,\n",
    "        device: str ='cuda'):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model, device=device))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "model = MambaClassifier(seq_len=seq_len, d_model=d_model, state_size=state_size, num_classes=num_classes, device=device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据集至DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)  # [num_samples, seq_len, d_model]\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)      # [num_samples]\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)      # [num_samples, seq_len, d_model]\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)          # [num_samples]\n",
    "\n",
    "# 将数据移动到设备\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class OCDDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "train_dataset = OCDDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = OCDDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练和验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # 清空梯度\n",
    "\n",
    "        # 前向推理\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新参数\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 计算准确率\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    # 计算训练损失和准确率\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = correct_predictions / total_samples\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['train_acc'].append(train_accuracy)\n",
    "\n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_val_predictions = 0\n",
    "    total_val_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            val_outputs = model(inputs)  # 在验证集上进行前向推理\n",
    "            val_loss = criterion(val_outputs, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "            # 计算验证准确率\n",
    "            _, predicted = torch.max(val_outputs, 1)\n",
    "            correct_val_predictions += (predicted == labels).sum().item()\n",
    "            total_val_samples += labels.size(0)\n",
    "\n",
    "    # 计算验证损失和准确率\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = correct_val_predictions / total_val_samples\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    history['val_acc'].append(val_accuracy)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "          f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, '\n",
    "          f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(90, 200))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
